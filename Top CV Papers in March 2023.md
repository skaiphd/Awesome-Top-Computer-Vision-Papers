# Top Computer Vision Papers in March 2023

Computer vision, a field of artificial intelligence focused on enabling machines to interpret and understand the visual world, is rapidly evolving with groundbreaking research and technological advancements. 

In March 2023, several top-tier academic conferences and journals showcased innovative research in computer vision, presenting exciting breakthroughs in various subfields such as image recognition, vision model optimization, generative adversarial networks (GANs), image segmentation, video analysis, and more. 

In this notebook, we will provide a comprehensive overview of the most significant papers published in March 2023, highlighting the latest research and advancements in computer vision. The papers can be in this 9 categories:

* [Image Recognition](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=1.%20Image%20Recognition)
* [Vision Model Optimization](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=2.%20Vision%20Model%20Optimization)
* [Generative Adversarial Networks (GANS)](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=3.%20Generative%20Adversarial%20Networks%C2%A0(GANS))
* [Image Segmentation](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=6.-,Image%20%26%20Video%20Editing,-Paper)
* [Video Analysis](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=on%20several%20datasets-,5.%20Video%20Analysis,-Paper)
* [Image & Video Editing](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=6.%20Image%20%26%20Video%20Editing)
* [Text to Image Generation](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=7.%20Text%20to%20Image%20Generation)
* [Image & Video Reconstruction](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=8.%20Image%20%26%20Video%20Reconstruction)
* [Action Recognition](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/Top%20CV%20Papers%20in%20March%202023.md#:~:text=9.%20Action%20Recognition)



<h4 align="left">1. Image Recognition </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[Your Diffusion Model is Secretly a Zero-Shot Classifier](https://arxiv.org/abs/2303.16203) | [:octocat:](https://diffusion-classifier.github.io) | In this paper, the authors show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. The  generative approach to classification, which they called Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, they found that this diffusion-based approach has stronger multimodal relational reasoning abilities than competing discriminative approaches.|
| [Prismer: A Vision-Language Model with An Ensemble of Experts](https://arxiv.org/abs/2303.02506) |[:octocat:](https://shikun.io/projects/prismer) |Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of diverse, pre-trained domain experts. Prismer achieves fine-tuned and few-shot learning vision-language reasoning performance which is competitive with the current state-of-the-art, whilst requiring up to two orders of magnitude less training data.|
| [A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision](https://arxiv.org/abs/2303.17376)  | -- | |
| [SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer](https://arxiv.org/abs/2303.17605)  | -- | |
| [Patches Are All You Need? — proposes ConvMixer](https://arxiv.org/abs/2201.09792)  | -- |A parameter-efficient fully-convolutional model which replaces self-attention and MLP layers in ViTs with less-expressive depthwise and pointwise convolutional layers. |


<h4 align="left">2. Vision Model Optimization </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [Scaling Vision-Language Models with Sparse Mixture of Experts](https://arxiv.org/abs/2303.07226) | -- |In this paper, the authors explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. |

<h4 align="left"> 3. Generative Adversarial Networks (GANS) </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces](https://arxiv.org/abs/2303.06146) |[:octocat:](https://www.mmlab-ntu.com/project/styleganex/) | In this paper, authors  propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. |
| [Consistency Models](https://arxiv.org/abs/2303.01469)|-- |The authors propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. |
|[Scaling up GANs for Text-to-Image Synthesis](https://mingukkang.github.io/GigaGAN/static/paper/gigagan_cvpr2023_compressed.pdf) |[:octocat:](https://mingukkang.github.io/GigaGAN/) |Can we scale up GANs to benefit from large datasets like LAION? In this paper, the authors found that naÏvely increasing the capacity of the StyleGAN architecture quickly becomes unstable. They introduced GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations. |


<h4 align="left">4. Image Segmentation </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations](https://arxiv.org/abs/2303.16891) |[:octocat:](https://vibashan.github.io/ovis-web/) |This paper proposes a manual-mask-free approach for open-vocabulary instance segmentation leveraging a pre-trained vision-language model. Specifically, the authors generate pseudo-masks annotations for objects of interest from image-caption pairs using pre-trained VLM and weakly-supervised proposal and segmentation networks. These generated pseudo-mask annotations are then used to train an instance segmentation model, completely eliminating the need for human-provided box-level or pixel-level annotations.  |
| [Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](https://arxiv.org/pdf/2303.04803.pdf) |[:octocat:](https://jerryxu.net/ODISE/) |Text-to-image diffusion models have shown the remarkable capability of generating high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. The authors propose to leverage the frozen representation of both these models to perform panoptic segmentation of any category in the wild. |
| [Open-world Instance Segmentation: Top-down Learning with Bottom-up Supervision](https://arxiv.org/abs/2303.05503) |[:octocat:](https://tarun005.github.io/UDOS/)|In this paper authors present UDOS for instance segmentation in an open world by leveraging weak supervision from unsupervised bottom-up segmentation algorithms like selective search. They  first predict part masks corresponding to objects, followed by affinity-based grouping and refinement modules to predict full-instance masks for both seen and unseen objects in an image. They achieved significantly better results compared to existing SOTA on open-world instance segmentation on several datasets |

<h4 align="left">5. Video Analysis </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [Unmasked Teacher: Towards Training-Efficient Video Foundation Models](https://arxiv.org/abs/2303.16058)|[:octocat:](https://github.com/OpenGVLab/unmasked_teacher)  | Video Foundation Models (VFMs) have received limited exploration due to high computational costs and data scarcity. Previous VFMs rely on Image Foundation Models (IFMs), which face challenges in transferring to the video domain. Although VideoMAE has trained a robust ViT from limited data, its low-level reconstruction poses convergence difficulties and conflicts with high-level cross-modal alignment. This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods. To increase data efficiency, they masked out most of the low-semantics video tokens, but selectively align the unmasked tokens with IFM, which serves as the UnMasked Teacher (UMT). By providing semantic guidance, our method enables faster convergence and multimodal friendliness. With a progressive pre-training framework, our model can handle various tasks including scene-related, temporal-related, and complex video-language understanding. Using only public sources for pre-training in 6 days on 32 A100 GPUs, this scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks.|

<h4 align="left">6. Image & Video Editing </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [Video-P2P: Video Editing with Cross-attention Control](https://arxiv.org/abs/2303.04761) | [:octocat:](https://video-p2p.github.io/) |This paper presents Video-P2P, a novel framework for real-world video editing with cross-attention control. While attention control has proven effective for image editing with pre-trained image generation models, there are currently no large-scale video generation models publicly available. Video-P2P addresses this limitation by adapting an image generation diffusion model to complete various video editing tasks. Specifically, they propose to first tune a Text-to-Set (T2S) model to complete an approximate inversion and then optimize a shared unconditional embedding to achieve accurate video inversion with a small memory cost. For attention control, the authors introduced a novel decoupled-guidance strategy, which uses different guidance strategies for the source and target prompts. The optimized unconditional embedding for the source prompt improves reconstruction ability, while an initialized unconditional embedding for the target prompt enhances editability. Incorporating the attention maps of these two branches enables detailed editing. These technical designs enable various text-driven editing applications, including word swap, prompt refinement, and attention re-weighting. Video-P2P works well on real-world videos for generating new characters while optimally preserving their original poses and scenes. It significantly outperforms previous approaches. |
| [Edit-A-Video: Single Video Editing with Object-Aware Consistency](https://arxiv.org/abs/2303.07945)  | [:octocat:](https://edit-a-video.github.io) |Despite the fact that the text-to-video (TTV) model has recently achieved remarkable success, there have been few approaches to TTV for its extension to video editing. Motivated by approaches on TTV models adapting from diffusion-based text-to-image (TTI) models, the authors suggest the video editing framework given only a pre-trained TTI model and a single <text, video> pair, which we term Edit-A-Video. The framework consists of two stages: (1) inflating the 2D model into the 3D model by appending temporal modules and tuning on the source video (2) inverting the source video into the noise and editing with target text prompt and attention map injection. Each stage enables the temporal modeling and preservation of semantic attributes of the source video. One of the key challenges for video editing includes a background inconsistency problem, where the regions not included in the edit suffer from undesirable and inconsistent temporal alterations. To mitigate this issue, the authors also introduce a novel mask-blending method, named sparse-causal blending (SC Blending). They improved the previous mask blending methods to reflect the temporal consistency so that the area where the editing is applied exhibits smooth transition while also achieving spatio-temporal consistency of the unedited regions. The authors present extensive experimental results over various types of text and videos and demonstrate the superiority of the proposed method compared to baselines in terms of background consistency, text alignment, and video editing quality. |
| [VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs](https://arxiv.org/abs/2303.15893)  |[:octocat:](http://afruehstueck.github.io/vive3D/) |The authors introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. The authors proposed two new building blocks. First, they introduced a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), they are the first to demonstrate edits that show novel views of the head enabled by the inherent properties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video. The experiments demonstrated that VIVE3D generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially consistent manner. |
| [StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing](https://arxiv.org/abs/2303.15649) | -- | |
| [MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path](https://arxiv.org/abs/2303.16765) | -- |Image generation using diffusion can be controlled in multiple ways. In this paper, the authors systematically analyze the equations of modern generative diffusion networks to propose a framework, called MDP, that explains the design space of suitable manipulations. They identify 5 different manipulations, including intermediate latent, conditional embedding, cross-attention maps, guidance, and predicted noise. They analyzed the corresponding parameters of these manipulations and the manipulation schedule. They showed that some previous editing methods fit nicely into their framework. Particularly, they identified one specific configuration as a new type of control by manipulating the predicted noise, which can perform higher-quality edits than previous work for a variety of local and global edits.|
| [StyO: Stylize Your Face in Only One-Shot](https://arxiv.org/abs/2303.03231) | -- | |
| [Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models](https://arxiv.org/abs/2303.17599) | -- | |


<h4 align="left">7. Text to Image Generation </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://arxiv.org/abs/2303.06555) |[:octocat:](https://github.com/thu-ml/unidiffuser) | This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model - perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. |
| [HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images](https://arxiv.org/abs/2303.16509) | -- | Diffusion models have emerged as the best approach for the generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. In the paper the authors address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. They evaluated the method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. They showed that the proposed diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling. |
| [Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion](https://arxiv.org/abs/2303.08767) | -- |Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, the authors present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. This method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, they demonstrated that this approach produces highly personalized and complex semantic image edits across a wide range of tasks. |
| [Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation](https://arxiv.org/abs/2302.13848)|[:octocat:](https://github.com/csyxwei/ELITE) |Given an image indicates the target concept (usually an object), the authors propose a learning-based encoder ELITE to encode the visual concept into the textual embeddings, which can be further flexibly composed into new scenes. It consists of two modules: 1. A global mapping network is first trained to encode a concept image into multiple textual word embeddings, where one primary word (w0) for a well-editable concept and other auxiliary words (w1···N) to exclude irrelevant disturbances. 2. A local mapping network is further trained, which projects the foreground object into textual feature space to provide local details. |
| [TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation](https://arxiv.org/abs/2303.04248) | -- |Denoising Diffusion models have demonstrated their proficiency for generative sampling. However, generating good samples often requires many iterations. Consequently, techniques such as binary time distillation (BTD) have been proposed to reduce the number of network calls for a fixed architecture. In this paper, the authors introduce TRAnsitive Closure Time-distillation (TRACT), a new method that extends BTD. For single-step diffusion,TRACT improves FID by up to 2.4x on the same architecture and achieves new single-step Denoising Diffusion Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for CIFAR10). |
| [Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles](https://arxiv.org/abs/2303.03751)|[:octocat:](https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback)| n this work, the authors invented a new zero-order optimization algorithm that can optimize any function via only its ranking oracle. More importantly, they successfully apply their algorithm to a novel application, where they optimized the latent embedding of Stable Diffusion with human ranking feedback. Specifically, starting from the latent embedding of an initial image, they first perturbed the embedding with multiple random noise vectors and then use Stable Diffusion to generate multiple similar images (only differ in details). Then they asked some human evaluator to rank those generated images. Finally, the algorithm will update the latent embedding based on the ranking information. Notice: this method does not require any training or finetuning at all! It usually takes around 10–20 rounds of ranking feedback before obtaining an image with satisfying details.  |
| [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671) |[:octocat:](https://github.com/microsoft/visual-chatgpt) | |
| [Cones: Concept Neurons in Diffusion Models for Customized Generation](https://arxiv.org/abs/2303.05125)  | -- | Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.|
| [Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation](https://arxiv.org/abs/2303.07937) |[:octocat:](https://ku-cvlab.github.io/3DFuse/) | Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation, a methodology of using pretrained text-to-2D diffusion models to optimize neural radiance field (NeRF) in the zero-shot setting. However, the lack of 3D awareness in the 2D diffusion models destabilizes score distillation-based methods from reconstructing a plausible 3D scene. To address this issue, the authors propose 3DFuse, a novel framework that incorporates 3D awareness into pretrained 2D diffusion models, enhancing the robustness and 3D consistency of score distillation-based methods. They realized this by first constructing a coarse 3D structure of a given text prompt and then utilizing projected, view-specific depth map as a condition for the diffusion model. Additionally, they introduce a training strategy that enables the 2D diffusion model learns to handle the errors and sparsity within the coarse 3D structure for robust generation, as well as a method for ensuring semantic consistency throughout all viewpoints of the scene. This framework surpasses the limitations of prior arts, and has significant implications for 3D consistent generation of 2D diffusion models.|
| [SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://arxiv.org/abs/2211.12194) |[:octocat:](https://github.com/Winfredy/SadTalker) | Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. Researahers argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. In this paper the authors present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, they explicitly model the connections between audio and different types of motion coefficients individually. Precisely, they present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, they design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video.|
| [High-resolution image reconstruction with latent diffusion models from human brain](https://www.biorxiv.org/content/10.1101/2022.11.18.517004v3) |  | -- |This paper proposes an approach for high-resolution image reconstruction with latent diffusion models from human brain activity.|
| [Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345) |[:octocat:](https://erasing.baulab.info/) | |
| [Word-As-Image for Semantic Typography](https://arxiv.org/abs/2303.01818) |[:octocat:](https://wordasimage.github.io/Word-As-Image-Page/) |A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability.|
| [Consistent View Synthesis with Pose-Guided Diffusion Models](https://arxiv.org/abs/2303.17598)  | -- |Novel view synthesis from a single image has been a cornerstone problem for many Virtual Reality applications that provide immersive experiences. However, most existing techniques can only synthesize novel views within a limited range of camera motion or fail to generate consistent and high-quality novel views under significant camera movement. In this work, the authors propose a pose-guided diffusion model to generate a consistent long-term video of novel views from a single image. They designed an attention layer that uses epipolar lines as constraints to facilitate the association between different viewpoints. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed diffusion model against state-of-the-art transformer-based and GAN-based approaches.|
| [Discriminative Class Tokens for Text-to-Image Diffusion Models](https://arxiv.org/abs/2303.17155) | -- | |
| [Token Merging for Fast Stable Diffusion](https://arxiv.org/abs/2303.17604)|[:octocat:](https://github.com/dbolya/tomesd) | |
| [Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models](https://arxiv.org/abs/2303.11989) |[:octocat:](https://lukashoel.github.io/text-to-room/) |This paper represents a method for generating room-scale textured 3D meshes from a given text prompt as input. To this end, this paper leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, the authors combine monocular depth estimation with a text-conditioned inpainting model. The core idea of this approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, they propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects or zoom-out trajectories from text, this method generates complete 3D scenes with multiple objects and explicit 3D geometry.|
| [SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger](https://arxiv.org/abs/2303.17561) | -- | |

<h4 align="left">8. Image & Video Reconstruction </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling](https://arxiv.org/abs/2303.02416) | -- | |
| [X-Avatar: Expressive Human Avatars](https://skype-line.github.io/projects/X-Avatar/) |[:octocat:](https://arxiv.org/abs/2303.04805) | |
| [Learning Object-Centric Neural Scattering Functions for Free-viewpoint Relighting and Scene Composition](https://arxiv.org/abs/2303.06138) | -- | |
| [NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer](https://arxiv.org/abs/2303.06919)  |[:octocat:](https://redrock303.github.io/nerflix/)  | |
| [NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views](https://arxiv.org/abs/2211.16431) | -- | |
| [MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices](https://arxiv.org/abs/2303.01932) |[:octocat:](https://github.com/ActiveVisionLab/MobileBrick) | |
| [3D Cinemagraphy from a Single Image](https://arxiv.org/abs/2303.05724) |[:octocat:](https://xingyi-li.github.io/3d-cinemagraphy/) | |
| [NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes](https://arxiv.org/abs/2303.09431) | -- |A compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach; distills NeRFs into geometrically-accurate 3D meshes.

 |



<h4 align="left">9. Action Recognition </h4>

| Paper  | Project Page | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [Vision-Language Models as Success Detectors](https://arxiv.org/abs/2303.07280)| -- | |
| [SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision](https://arxiv.org/abs/2303.17200) | -- | |

