# Montly-Top-Computer-Vision-Papers

Top computer vision papers published by the scientific community every month. 

* [Top Computer Vision Papers in April 2023](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/README.md#:~:text=Top%20Computer%20Vision%20Papers%20in%20April%202023%3A)
* [Top Computer Vision Papers in March 2023](https://github.com/youssefHosni/Montly-Top-Computer-Vision-Papers/blob/main/README.md#:~:text=Top%20Computer%20Vision%20Papers%20in%20March%202023%3A)

<h2 align="left">Top Computer Vision Papers in April 2023:</h2>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| []()  |  | |
| []()  | | |
| []()  | | |
| []()  | | |


<h2 align="left">Top Computer Vision Papers in March 2023:</h2>




<h4 align="left">Image Classification:</h4>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[Your Diffusion Model is Secretly a Zero-Shot Classifier](https://arxiv.org/abs/2303.16203) | [:octocat:](https://diffusion-classifier.github.io) | In this paper, the authors show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. The  generative approach to classification, which they called Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, they found that this diffusion-based approach has stronger multimodal relational reasoning abilities than competing discriminative approaches.|
| []()  | | |
| []()  | | |
| []()  | | |


<h4 align="left">Vision Model Optimization:</h4>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
| [Scaling Vision-Language Models with Sparse Mixture of Experts](https://arxiv.org/abs/2303.07226) | -- |In this paper, the authors explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. |
| []()  | | |
| []()  | | |




<h4 align="left">Text to Image Generation:</h4>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://arxiv.org/abs/2303.06555) |[:octocat:](https://github.com/thu-ml/unidiffuser) | This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model - perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. |
| []()  |  | |
| []()  | | |
| []()  | | |
| []()  | | |

<h4 align="left">Image Segmentation:</h4>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations](https://arxiv.org/abs/2303.16891) |[:octocat:](https://vibashan.github.io/ovis-web/) |This paper proposes a manual-mask-free approach for open-vocabulary instance segmentation leveraging a pre-trained vision-language model. Specifically, the authors generate pseudo-masks annotations for objects of interest from image-caption pairs using pre-trained VLM and weakly-supervised proposal and segmentation networks. These generated pseudo-mask annotations are then used to train an instance segmentation model, completely eliminating the need for human-provided box-level or pixel-level annotations.  |
| []()  |  | |
| []()  | | |
| []()  | | |
| []()  | | |

<h4 align="left"> Generative Adversarial Networks (GANS) </h4>

| Paper  | Links | Abstract / Short Summary |
| ------------- | ------------- |------------- |
|[StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces](https://arxiv.org/abs/2303.06146) |[:octocat:](https://www.mmlab-ntu.com/project/styleganex/) | In this paper, authors  propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. |
| [Consistency Models](https://arxiv.org/abs/2303.01469)|-- |The authors propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. |
| []()  | | |
| []()  | | |
| []()  | | |
